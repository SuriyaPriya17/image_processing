{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9tqdPXfHgog+JoGoB4ryU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuriyaPriya17/image_processing/blob/main/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkqLxNhQ4EFa",
        "outputId": "7d56dcb0-b76c-41ba-e68c-897afb0e87bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the Siamese network (dummy data)...\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 436ms/step - loss: 1.2532\n",
            "\n",
            "Evaluating similarity with the trained embedding network...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Similarity between img1 and img2 (same class): [-0.9966147]\n",
            "Similarity between img1 and img3 (different class): [-0.9970206]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import numpy as np\n",
        "\n",
        "# 1. Data Preparation - Create pairs of images\n",
        "def make_pairs(images, labels):\n",
        "    pair_images = []\n",
        "    pair_labels = []\n",
        "    num_classes = len(np.unique(labels))\n",
        "    idx = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "\n",
        "    for anchor_idx in range(len(images)):\n",
        "        current_image = images[anchor_idx]\n",
        "        current_label = labels[anchor_idx]\n",
        "\n",
        "        # Positive pair\n",
        "        positive_idx = np.random.choice(idx[current_label])\n",
        "        while positive_idx == anchor_idx:\n",
        "            positive_idx = np.random.choice(idx[current_label])\n",
        "        positive_image = images[positive_idx]\n",
        "\n",
        "        pair_images.append([current_image, positive_image])\n",
        "        pair_labels.append(1)\n",
        "\n",
        "        # Negative pair\n",
        "        negative_label = np.random.randint(0, num_classes)\n",
        "        while negative_label == current_label:\n",
        "            negative_label = np.random.randint(0, num_classes)\n",
        "        negative_image = images[np.random.choice(idx[negative_label])]\n",
        "\n",
        "        pair_images.append([current_image, negative_image])\n",
        "        pair_labels.append(0)\n",
        "\n",
        "    return np.array(pair_images), np.array(pair_labels)\n",
        "\n",
        "\n",
        "# 2. Build the Siamese Network with Transfer Learning\n",
        "def build_siamese_model(input_shape, embedding_dim=128):\n",
        "    base_cnn = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_cnn.trainable = False\n",
        "\n",
        "    input_tensor = keras.Input(shape=input_shape)\n",
        "    x = base_cnn(input_tensor, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(embedding_dim, activation=\"relu\")(x)\n",
        "    embedding_network = keras.Model(input_tensor, x, name=\"embedding_network\")\n",
        "\n",
        "    # Define Siamese inputs\n",
        "    input_a = keras.Input(shape=input_shape, name=\"anchor\")\n",
        "    input_p = keras.Input(shape=input_shape, name=\"positive\")\n",
        "    input_n = keras.Input(shape=input_shape, name=\"negative\")\n",
        "\n",
        "    # Get embeddings\n",
        "    embedding_a = embedding_network(input_a)\n",
        "    embedding_p = embedding_network(input_p)\n",
        "    embedding_n = embedding_network(input_n)\n",
        "\n",
        "    # Use a Lambda layer to stack embeddings safely\n",
        "    output = layers.Lambda(lambda tensors: tf.stack(tensors, axis=1))([embedding_a, embedding_p, embedding_n])\n",
        "\n",
        "    siamese_model = keras.Model(inputs=[input_a, input_p, input_n], outputs=output, name=\"siamese_model\")\n",
        "    return siamese_model, embedding_network\n",
        "\n",
        "\n",
        "# 3. Define Triplet Loss\n",
        "def triplet_loss(y_true, y_pred, margin=1.0):\n",
        "    anchor, positive, negative = tf.unstack(y_pred, num=3, axis=1)\n",
        "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "    loss = tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "    x_train = x_train[:1000]\n",
        "    y_train = y_train[:1000]\n",
        "    x_test = x_test[:200]\n",
        "    y_test = y_test[:200]\n",
        "\n",
        "    input_shape = (32, 32, 3)\n",
        "    x_train = tf.image.resize(x_train, (input_shape[0], input_shape[1])) / 255.0\n",
        "    x_test = tf.image.resize(x_test, (input_shape[0], input_shape[1])) / 255.0\n",
        "    y_train = y_train.flatten()\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "    siamese_model, embedding_network = build_siamese_model(input_shape)\n",
        "\n",
        "    siamese_model.compile(optimizer=keras.optimizers.Adam(1e-4),\n",
        "                          loss=triplet_loss)\n",
        "\n",
        "    # Dummy triplets for demonstration\n",
        "    dummy_anchor = x_train[:400]\n",
        "    # Create dummy_positive and dummy_negative with the same number of samples as dummy_anchor\n",
        "    dummy_positive = x_train[100:500]\n",
        "    dummy_negative = x_train[200:600]\n",
        "    dummy_y = np.zeros(len(dummy_anchor))\n",
        "\n",
        "    print(\"Training the Siamese network (dummy data)...\")\n",
        "    siamese_model.fit([dummy_anchor, dummy_positive, dummy_negative],\n",
        "                      dummy_y, epochs=1)\n",
        "\n",
        "    print(\"\\nEvaluating similarity with the trained embedding network...\")\n",
        "    img1 = x_test[0]\n",
        "    img2_same_class = x_test[1]\n",
        "    img3_diff_class = x_test[2]\n",
        "\n",
        "    emb1 = embedding_network.predict(np.expand_dims(img1, axis=0))\n",
        "    emb2 = embedding_network.predict(np.expand_dims(img2_same_class, axis=0))\n",
        "    emb3 = embedding_network.predict(np.expand_dims(img3_diff_class, axis=0))\n",
        "\n",
        "    similarity_same = tf.keras.losses.cosine_similarity(emb1, emb2).numpy()\n",
        "    similarity_diff = tf.keras.losses.cosine_similarity(emb1, emb3).numpy()\n",
        "\n",
        "    print(f\"Similarity between img1 and img2 (same class): {similarity_same}\")\n",
        "    print(f\"Similarity between img1 and img3 (different class): {similarity_diff}\")"
      ]
    }
  ]
}